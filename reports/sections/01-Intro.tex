\section{Introduction}
In this project we will apply normalizing flows in the realm of variational inference.
So to give some context to our endeavour we will start out by motivating variational inference and normalizing flows.\footnote{\url{https://github.com/silasbrack/normalizing-flows}}

\subsection{Variational inference}

In variational inference we strive to approximate some distribution by an element in some distributional family.

The reason we might wish to do so is often that we have some implicit distribution which we do not have an analytical expression for but we might be able to sample form it by use of some sampling scheme like MCMC chains.

in our case this we have an implicit distribution which arises from forming the posterior form some non conjugate prior-likelihood pair:
\begin{align*}
    p(z|\mathcal{D})=\frac{p(\mathcal{D}|z)p(z)}{p(\mathcal{D})}
\end{align*}
Here we have two annoyances, one being the intractability of the integral in $p(\mathcal{D})=\int p(\mathcal{D}|z)p(z)dz$ and the other being annoyance of the non-parametric functional form of: $p(z|\mathcal{D})\propto \prod_n p(d_n|z)p(z)$
which becomes excessively cumbersome for inference on large data sets. Thus this incentivises us to find a function which can approximate $p(z|\mathcal{D})\approx q_\theta(z)$ as this would allow us to sample directly and independently from $q\theta$ and have $q_\theta$ be a parametric distribution and thus potentially much less cumbersome.
However the effectiveness of this approach all depends on how well $q_\theta(z)$ approximates $p(z|\mathcal{D})$.
This necessitates rich distributional families.
and this is where the frame work of normalizing flows seems particular promising.
That is normalizing flows gives us a general frame work for constructing arbitrarily complex distributional families.
The general idea is to start out with simple distribution in the sense that they are easy to sample from, then we apply a series of transformations to end up with a possibly much more complex distribution, but one which we can still sample independently from as we know its relation to a sampleable distribution.

With this we are ready to start developing normalizing flows further and start to look at how we can use them to approximate arbitrary distributions.

Letting $p(z|y) \approx q(z)$, we calculate the posterior predictive as:
\begin{align*}
    p(y^*|y) = \int p(y^*|z) p(z | y) dz  \\
    p(y^*|y) \approx \int p(y^*|z) q(z) dz
\end{align*}

\clearpage
\section{Normalising Flows}\label{NF}
As previously mentioned a normalizing flow can be defined by a sampleable base distribution along with a series of diffeomorphisms:
\begin{align*}
    z_0\sim q(z_0) && f_{(n)}=f_n\circ f_{n-1} \circ \cdots \circ f_1
\end{align*}
and well denote the $z_0$s image through $f_{(n)}$ as $z_n$, ie $f_{(n)}(z_0)=z_n$.\\
We can then recursively apply the change of variable formula:
\begin{align*}
    p(z')=p(z)|\det(J(f^{-1}))|=p(z)|\det(J(f\circ z))|^{-1} \ \ \ \  \text{where} \ \ \ \ z'=f(z)
\end{align*}
to derive $q(z_n)$, here the last equality is due to the choice of diffeomorphisms. Through recursive application over all $f_i$ we derive:
\begin{align*}
    q(z_n)=q(z_0)\prod_k|\det J(f_k \circ z_{k-1})|^{-1}.
\end{align*}
This can be viewed as parametric distributional family parameterized by the choice of $q(z_0)$ as well as the choice of $f_{(n)}$ whose parameters we will denote $\theta=\{\theta_n,\theta_{n-1},\cdots,\theta_1\}$
\cite{gelman2013bayesian}
